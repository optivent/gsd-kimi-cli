# Phase 2 Plan: Core Engine & Chat System

## Goal
Build working chat engine with message streaming, context management, and LLM integration.

## Phase Overview
This is the "proof of life" phase. By the end, you can have a real conversation with the system. Everything else builds on this foundation.

---

## Task 1: Session Management
**Status:** ⏳ Ready  
**Assignee:** gsd-executor  
**Priority:** CRITICAL  
**Duration:** 3-4 days

### Subtasks

#### 1.1 Session GenServer
```elixir
defmodule KimiCore.Session do
  use GenServer, restart: :temporary
  
  defstruct [
    :id,
    :messages,
    :context,
    :metadata,
    :created_at,
    :last_activity
  ]
  
  # API
  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: via_tuple(opts[:id]))
  end
  
  def get_state(session_id) do
    GenServer.call(via_tuple(session_id), :get_state)
  end
  
  def add_message(session_id, message) do
    GenServer.call(via_tuple(session_id), {:add_message, message})
  end
  
  # Callbacks
  @impl true
  def init(opts) do
    state = %__MODULE__{
      id: opts[:id],
      messages: [],
      context: opts[:context] || %{},
      metadata: opts[:metadata] || %{},
      created_at: DateTime.utc_now(),
      last_activity: DateTime.utc_now()
    }
    
    # Persist to ETS for recovery
    :ets.insert(:session_cache, {state.id, state})
    
    {:ok, state}
  end
end
```

- [ ] Session state structure
- [ ] GenServer implementation
- [ ] Process registry (via Horde or Registry)
- [ ] ETS persistence
- [ ] Session recovery on crash

#### 1.2 Session Supervisor
```elixir
defmodule KimiCore.Session.Supervisor do
  use DynamicSupervisor
  
  def start_session(opts) do
    spec = {KimiCore.Session, opts}
    DynamicSupervisor.start_child(__MODULE__, spec)
  end
  
  def terminate_session(session_id) do
    case Registry.lookup(KimiCore.SessionRegistry, session_id) do
      [{pid, _}] -> DynamicSupervisor.terminate_child(__MODULE__, pid)
      [] -> {:error, :not_found}
    end
  end
end
```

- [ ] DynamicSupervisor for sessions
- [ ] Graceful termination
- [ ] Resource limits (max sessions)
- [ ] Cleanup of old sessions

#### 1.3 Session API
- [ ] Create session
- [ ] Get session state
- [ ] Update context
- [ ] Archive session
- [ ] List active sessions

### Deliverables
- `kimi_core/lib/session.ex`
- `kimi_core/lib/session/supervisor.ex`
- Session persistence
- Recovery mechanism

### Verification
- [ ] Sessions survive process restarts
- [ ] Multiple concurrent sessions work
- [ ] Old sessions auto-cleanup
- [ ] State properly persisted

---

## Task 2: LLM Gateway
**Status:** ⏳ Ready  
**Assignee:** gsd-executor  
**Priority:** CRITICAL  
**Duration:** 4-5 days

### Subtasks

#### 2.1 Provider Abstraction
```elixir
defmodule KimiCore.LLM.Provider do
  @callback chat(messages :: list(), opts :: keyword()) ::
    {:ok, String.t()} | {:error, term()}
  
  @callback stream(messages :: list(), opts :: keyword()) ::
    {:ok, Enumerable.t()} | {:error, term()}
end

defmodule KimiCore.LLM.Provider.Kimi do
  @behaviour KimiCore.LLM.Provider
  
  def chat(messages, opts) do
    # HTTP call to Kimi API
  end
  
  def stream(messages, opts) do
    # HTTP streaming
  end
end
```

- [ ] Provider behaviour
- [ ] Kimi implementation
- [ ] OpenAI compatibility
- [ ] Provider selection

#### 2.2 Connection Pooling
```elixir
defmodule KimiCore.LLM.Pool do
  use GenServer
  
  # Manage connection pool to LLM APIs
  # Handle rate limiting
  # Retry logic with backoff
  # Circuit breaker pattern
end
```

- [ ] Poolboy for connection pooling
- [ ] Rate limiting (token bucket)
- [ ] Exponential backoff retry
- [ ] Circuit breaker (fuse)

#### 2.3 Streaming Implementation
```elixir
defmodule KimiCore.LLM.Stream do
  def stream_completion(messages, opts \\ []) do
    provider = opts[:provider] || default_provider()
    
    Stream.resource(
      fn -> init_stream(provider, messages, opts) end,
      &next_chunk/1,
      &cleanup/1
    )
  end
end
```

- [ ] Elixir Stream for chunks
- [ ] Backpressure handling
- [ ] Cancellation support
- [ ] Error mid-stream handling

#### 2.4 Caching Layer
```elixir
defmodule KimiCore.LLM.Cache do
  # Cache responses for identical requests
  # TTL-based expiration
  # Configurable cache size
end
```

- [ ] Request hashing
- [ ] ETS-based cache
- [ ] TTL configuration
- [ ] Cache invalidation

### Deliverables
- `kimi_core/lib/llm/provider.ex`
- `kimi_core/lib/llm/pool.ex`
- `kimi_core/lib/llm/stream.ex`
- `kimi_core/lib/llm/cache.ex`
- Provider implementations

### Verification
- [ ] Streaming works end-to-end
- [ ] Rate limiting respected
- [ ] Retries happen on failure
- [ ] Cache hits improve performance

---

## Task 3: Context Management
**Status:** ⏳ Ready  
**Assignee:** gsd-executor  
**Priority:** HIGH  
**Duration:** 3-4 days

### Subtasks

#### 3.1 Context Building
```elixir
defmodule KimiCore.Context do
  defstruct [
    :messages,
    :files,
    :skills,
    :system_prompt,
    :metadata
  ]
  
  def build(session, opts \\ []) do
    %__MODULE__{
      messages: session.messages,
      files: load_relevant_files(opts[:files]),
      skills: load_available_skills(),
      system_prompt: build_system_prompt(opts),
      metadata: build_metadata(session)
    }
  end
end
```

- [ ] Context structure
- [ ] Message formatting
- [ ] File loading
- [ ] Token counting

#### 3.2 Token Management
```elixir
defmodule KimiCore.Context.Tokens do
  def count(context) do
    # Estimate tokens for context
  end
  
  def truncate(context, max_tokens) do
    # Smart truncation preserving important context
  end
end
```

- [ ] Token estimation
- [ ] Context window management
- [ ] Smart truncation (keep recent, summarize old)
- [ ] Warnings for large contexts

#### 3.3 File Context
```elixir
defmodule KimiCore.Context.Files do
  def load_files(paths) do
    # Read files with caching
    # Handle large files (chunking)
    # Syntax highlighting hints
  end
  
  def find_relevant_files(query, opts) do
    # Semantic search over codebase
    # Grep-based search
    # Recent file tracking
  end
end
```

- [ ] File reading with caching
- [ ] Large file handling
- [ ] Relevance scoring
- [ ] Tree-sitter integration (optional)

### Deliverables
- `kimi_core/lib/context.ex`
- `kimi_core/lib/context/tokens.ex`
- `kimi_core/lib/context/files.ex`
- Context building pipeline

### Verification
- [ ] Context properly formatted for LLM
- [ ] Token limits respected
- [ ] Files loaded efficiently
- [ ] Context survives session restart

---

## Task 4: Basic UI (Terminal)
**Status:** ⏳ Ready  
**Assignee:** gsd-executor  
**Priority:** HIGH  
**Duration:** 3-4 days

### Subtasks

#### 4.1 Terminal UI Framework
```elixir
defmodule KimiUI.Terminal do
  use Ratatouille.App
  
  def init(_context) do
    %{
      session_id: nil,
      messages: [],
      input: "",
      status: :ready
    }
  end
  
  def update(model, msg) do
    case msg do
      {:key, key} -> handle_key(model, key)
      {:message, content} -> add_message(model, content)
      _ -> model
    end
  end
  
  def render(model) do
    view do
      panel(title: "Kimi-GSD-EX") do
        viewport do
          # Messages
        end
        
        panel(height: 3) do
          # Input area
        end
        
        panel(height: 1) do
          # Status bar
        end
      end
    end
  end
end
```

- [ ] Ratatouille setup
- [ ] Basic layout (messages, input, status)
- [ ] Event handling
- [ ] Rendering pipeline

#### 4.2 Message Display
- [ ] Message formatting
- [ ] Syntax highlighting (ex_unicode)
- [ ] Code blocks
- [ ] Markdown rendering

#### 4.3 Input Handling
- [ ] Multi-line input
- [ ] History (up/down arrows)
- [ ] Auto-completion hints
- [ ] Key bindings

#### 4.4 Status Bar
```elixir
defmodule KimiUI.StatusBar do
  def render(status) do
    # Session info
    # LLM connection status
    # Context usage
    # Any errors/warnings
  end
end
```

- [ ] Connection status
- [ ] Context size
- [ ] Current mode

### Deliverables
- `kimi_ui/lib/terminal.ex`
- `kimi_ui/lib/terminal/render.ex`
- Working terminal UI

### Verification
- [ ] UI renders without errors
- [ ] Can type and send messages
- [ ] Messages display correctly
- [ ] Status bar shows info

---

## Integration: End-to-End Chat

### Put It All Together
```elixir
# User types message in UI
# ↓
# UI sends to Chat Engine
# ↓
# Chat Engine:
#   - Saves to session
#   - Builds context
#   - Calls LLM Gateway
# ↓
# LLM Gateway streams response
# ↓
# UI displays streaming response
# ↓
# Response saved to session
```

### Acceptance Criteria
- [ ] Can start new session
- [ ] Can type and send message
- [ ] See streaming LLM response
- [ ] Context maintained across messages
- [ ] Session persists restart

---

## Verification Criteria

- [ ] Basic chat works end-to-end
- [ ] Sessions survive crashes
- [ ] LLM streaming functional
- [ ] Context management works
- [ ] Terminal UI usable

## Dependencies
- Phase 1: Project Bootstrap (complete)

## Time Budget
- 13-17 days
- 2 developers

## Definition of Done
**"Can have a 10-message conversation with context awareness"**
